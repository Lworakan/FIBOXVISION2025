{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ./output/4_meter.csv\n",
      "Processed ./output/4_2_meter.csv\n",
      "Processed ./output/5_meter.csv\n",
      "Processed ./output/5_5_meter.csv\n",
      "Processed ./output/6_meter.csv\n",
      "Processed ./output/6_5_meter.csv\n",
      "Processed ./output/7_meter.csv\n",
      "Processed ./output/7_5_meter.csv\n",
      "\n",
      "Successfully merged 8 files into ./output/merged_PLC_tracking.csv\n",
      "Total rows in merged file: 22974\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define ground truth values corresponding to each file\n",
    "# ground_truth_values = ['3.5', '4.0', '4.5', '5.0', '5.5', '6.0', '6.5', '7.0', '7.5']\n",
    "\n",
    "ground_truth_values = ['4.0', '4.0', '4.5', '5.0', '5.5', '6.0', '6.5', '7.0', '7.5']\n",
    "# Input directory and file names\n",
    "input_dir = './output'\n",
    "# file_names = [\n",
    "#     'realsense_detections3500', 'realsense_detections4000', 'realsense_detections4500',\n",
    "#     'realsense_detections5000', 'realsense_detections5500', 'realsense_detections6000',\n",
    "#     'realsense_detections6500', 'realsense_detections7000', 'realsense_detections7500'\n",
    "# ]\n",
    "\n",
    "file_names = ['4_meter', '4_2_meter', '5_meter', \n",
    "              '5_5_meter', '6_meter', '6_5_meter',\n",
    "              '7_meter', '7_5_meter']\n",
    "# Output file for the merged data\n",
    "output_file = os.path.join(input_dir, 'merged_PLC_tracking.csv')\n",
    "\n",
    "# Step 1: First update each file with its ground truth value\n",
    "all_dataframes = []\n",
    "\n",
    "for idx, base_name in enumerate(file_names):\n",
    "    csv_file_path = os.path.join(input_dir, base_name + '.csv')\n",
    "    ground_truth = ground_truth_values[idx]\n",
    "    \n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"File not found: {csv_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Add ground truth column if it doesn't exist\n",
    "        if 'ground_truth' not in df.columns:\n",
    "            df['ground_truth'] = ground_truth\n",
    "        \n",
    "        # Add source filename column for tracking\n",
    "        df['source_file'] = base_name\n",
    "        \n",
    "        # Append to our list of dataframes\n",
    "        all_dataframes.append(df)\n",
    "        \n",
    "        print(f\"Processed {csv_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file_path}: {str(e)}\")\n",
    "\n",
    "# Step 2: Concatenate all dataframes\n",
    "if all_dataframes:\n",
    "    merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Step 3: Save the merged dataframe to a new CSV file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nSuccessfully merged {len(all_dataframes)} files into {output_file}\")\n",
    "    print(f\"Total rows in merged file: {len(merged_df)}\")\n",
    "else:\n",
    "    print(\"No data to merge. Please check the input files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# input_dir = '../Callback'\n",
    "\n",
    "# ground_truth_values = ['9','8.75','8.5','8','7.75', '7.5','7.25', '7','6.75','6.5', '6.25', '6','5.75', '5.5', '5.25','5', '4.75', '4.5', '4.25', '4']\n",
    "\n",
    "# input_dir = '../Callback'\n",
    "# file_names = ['Day2_9_depth_measurements','Day2_8_7_5_depth_measurements','Day2_8_5_depth_measurements' , 'Day2_8_depth_measurements', 'Day2_7_7_5_depth_measurements', 'Day2_7_5_depth_measurements', 'Day2_7_2_5_depth_measurements','Day2_7_depth_measurements', 'Day2_6_7_5_depth_measurements', 'Day2_6_5_depth_measurements', 'Day2_6_2_5_depth_measurements', 'Day2_6_depth_measurements'\n",
    "#               , 'Day2_5_7_5_depth_measurements', 'Day2_5_5_depth_measurements', 'Day2_5_2_5_depth_measurements', 'Day2_5_depth_measurements', 'Day2_4_7_5_depth_measurements', 'Day2_4_5_depth_measurements', 'Day2_4_2_5_depth_measurements', 'Day2_4_depth_measurements']\n",
    "\n",
    "# Ensure the number of filenames matches the number of ground truth values\n",
    "if len(file_names) != len(ground_truth_values):\n",
    "    raise ValueError(\"Mismatch between number of filenames and ground truth values.\")\n",
    "\n",
    "# headers = [\n",
    "#     'timestamp', 'x', 'y', 'w','h','depth',\n",
    "#     'gyro_data.x', 'gyro_data.y', 'gyro_data.z',\n",
    "#     'accel_data.x', 'accel_data.y', 'accel_data.z',\n",
    "#     'ground_truth'\n",
    "# ]\n",
    "\n",
    "\n",
    "# List to store all dataframes for merging\n",
    "all_dataframes = []\n",
    "\n",
    "# Process each file\n",
    "for idx, base_name in enumerate(file_names):\n",
    "    txt_file_path = os.path.join(input_dir, base_name + '.txt')\n",
    "    ground_truth = ground_truth_values[idx]  # Assign corresponding ground truth value\n",
    "\n",
    "    if not os.path.exists(txt_file_path):\n",
    "        print(f\"File not found: {txt_file_path}\")\n",
    "        continue\n",
    "    # Read the .txt file and prepare data for the .csv\n",
    "    csv_data = []\n",
    "    with open(txt_file_path, 'r') as txt_file:\n",
    "        for line in txt_file:\n",
    "            # Assuming each line contains 10 comma-separated values\n",
    "            data = line.strip().split(',')\n",
    "            if len(data) == 12:  # Ensure correct data format\n",
    "                data.append(ground_truth)  # Add the ground truth value\n",
    "                csv_data.append(data)\n",
    "            else:\n",
    "                print(f\"Skipping line in {txt_file_path} due to unexpected format: {line.strip()}\")\n",
    "\n",
    "    # Define the output .csv file path\n",
    "    csv_file_path = os.path.join(input_dir, base_name + '.csv')\n",
    "\n",
    "    # Write data to the .csv file\n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(headers)  # Write headers\n",
    "        writer.writerows(csv_data)  # Write data rows\n",
    "\n",
    "    print(f\"Converted: {txt_file_path} â†’ {csv_file_path} with ground_truth = {ground_truth}\")\n",
    "\n",
    "    # Read the CSV file into a dataframe\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    all_dataframes.append(df)\n",
    "\n",
    "# Merge all CSVs into one dataframe\n",
    "merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Define the output merged CSV file path\n",
    "merged_csv_path = os.path.join(input_dir, 'merged_output.csv')\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv(merged_csv_path, index=False)\n",
    "\n",
    "print(f\"All files successfully merged into {merged_csv_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data) == 10:  # Ensure correct data format\n",
    "    data.append(ground_truth)  # Add the ground truth value\n",
    "    csv_data.append(data)\n",
    "else:\n",
    "    print(f\"Skipping line in {txt_file_path} due to unexpected format: {line.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define ground truth values corresponding to each file\n",
    "# ground_truth_values = ['8', '7.5', '7', '6.5', '6', '5.5', '5', '4.5', '4', '3.5', '3']\n",
    "\n",
    "# # List of filenames without extensions\n",
    "# file_names = [\n",
    "#     'm8_1_depth_measurements', '7.5_depth_measurements', '7_depth_measurements', \n",
    "#     'm_65_depth_measurements', 'm6_depth_measurements', 'm_5_5depth_measurements', \n",
    "#     'm_5_depth_measurements', 'm_4_5_depth_measurements', 'm_4_depth_measurements', \n",
    "#     'm3_5_depth_measurements', 'm_3_depth_measurements'\n",
    "# ]\n",
    "\n",
    "# # Define the input directory\n",
    "# input_dir = '../Callback'\n",
    "\n",
    "# # Ensure the number of filenames matches the number of ground truth values\n",
    "# if len(file_names) != len(ground_truth_values):\n",
    "#     raise ValueError(\"Mismatch between number of filenames and ground truth values.\")\n",
    "\n",
    "# # Define the headers\n",
    "# headers = [\n",
    "#     'timestamp', 'x', 'y', 'depth',\n",
    "#     'gyro_data.x', 'gyro_data.y', 'gyro_data.z',\n",
    "#     'accel_data.x', 'accel_data.y', 'accel_data.z',\n",
    "#     'ground_truth', 'error'\n",
    "# ]\n",
    "\n",
    "# # List to store all dataframes for merging\n",
    "# all_dataframes = []\n",
    "\n",
    "# # Process each file\n",
    "# for idx, base_name in enumerate(file_names):\n",
    "#     txt_file_path = os.path.join(input_dir, base_name + '.txt')\n",
    "#     ground_truth = float(ground_truth_values[idx])  # Assign corresponding ground truth value\n",
    "\n",
    "#     if not os.path.exists(txt_file_path):\n",
    "#         print(f\"File not found: {txt_file_path}\")\n",
    "#         continue\n",
    "\n",
    "#     # Read the .txt file and prepare data for the .csv\n",
    "#     csv_data = []\n",
    "#     with open(txt_file_path, 'r') as txt_file:\n",
    "#         for line in txt_file:\n",
    "#             # Assuming each line contains 10 comma-separated values\n",
    "#             data = line.strip().split(',')\n",
    "#             if len(data) == 10:  # Ensure correct data format\n",
    "#                 depth_value = float(data[3])  # Convert depth to float\n",
    "#                 error_value = depth_value - ground_truth  # Calculate error\n",
    "#                 data.append(str(ground_truth))  # Add the ground truth value\n",
    "#                 data.append(str(error_value))  # Add the error value\n",
    "#                 csv_data.append(data)\n",
    "#             else:\n",
    "#                 print(f\"Skipping line in {txt_file_path} due to unexpected format: {line.strip()}\")\n",
    "\n",
    "#     # Define the output .csv file path\n",
    "#     csv_file_path = os.path.join(input_dir, base_name + '.csv')\n",
    "\n",
    "#     # Write data to the .csv file\n",
    "#     with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "#         writer.writerow(headers)  # Write headers\n",
    "#         writer.writerows(csv_data)  # Write data rows\n",
    "\n",
    "#     print(f\"Converted: {txt_file_path} â†’ {csv_file_path} with ground_truth = {ground_truth}\")\n",
    "\n",
    "#     # Read the CSV file into a dataframe\n",
    "#     df = pd.read_csv(csv_file_path)\n",
    "#     all_dataframes.append(df)\n",
    "\n",
    "# # Merge all CSVs into one dataframe\n",
    "# merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# # Define the output merged CSV file path\n",
    "# merged_csv_path = os.path.join(input_dir, 'merged_output.csv')\n",
    "\n",
    "# # Save the merged dataframe to a CSV file\n",
    "# merged_df.to_csv(merged_csv_path, index=False)\n",
    "\n",
    "# print(f\"All files successfully merged into {merged_csv_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Load JSON data from file with error handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_detections_to_rows(data, source_file):\n",
    "    \"\"\"Convert detection data to rows for DataFrame\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    if not data or 'detections' not in data:\n",
    "        return rows\n",
    "    \n",
    "    for detection in data.get('detections', []):\n",
    "        row = {\n",
    "            # Source file info\n",
    "            'source_file': source_file,\n",
    "            'sample_id': data.get('sample_id'),\n",
    "            'timestamp': data.get('timestamp'),\n",
    "            'session_start': data.get('session_start'),\n",
    "            'sample_count': data.get('sample_count'),\n",
    "            \n",
    "            # Camera info\n",
    "            'camera_width': data.get('camera_info', {}).get('width'),\n",
    "            'camera_height': data.get('camera_info', {}).get('height'),\n",
    "            'depth_scale': data.get('camera_info', {}).get('depth_scale'),\n",
    "            'origin_x': data.get('camera_info', {}).get('origin_x'),\n",
    "            'origin_y': data.get('camera_info', {}).get('origin_y'),\n",
    "            \n",
    "            # Detection info\n",
    "            'detection_id': detection.get('detection_id'),\n",
    "            'class_id': detection.get('class_id'),\n",
    "            'confidence': detection.get('confidence'),\n",
    "            \n",
    "            # Bounding box\n",
    "            'bbox_x1': detection.get('bounding_box', {}).get('x1'),\n",
    "            'bbox_y1': detection.get('bounding_box', {}).get('y1'),\n",
    "            'bbox_x2': detection.get('bounding_box', {}).get('x2'),\n",
    "            'bbox_y2': detection.get('bounding_box', {}).get('y2'),\n",
    "            'bbox_center_x': detection.get('bounding_box', {}).get('center_x'),\n",
    "            'bbox_center_y': detection.get('bounding_box', {}).get('center_y'),\n",
    "            'bbox_width': detection.get('bounding_box', {}).get('width'),\n",
    "            'bbox_height': detection.get('bounding_box', {}).get('height'),\n",
    "            \n",
    "            # Relative coordinates\n",
    "            'rel_x': detection.get('relative_coordinates', {}).get('rel_x'),\n",
    "            'rel_y': detection.get('relative_coordinates', {}).get('rel_y'),\n",
    "            \n",
    "            # Depth statistics\n",
    "            'mean_depth': detection.get('depth_information', {}).get('statistics', {}).get('mean_depth'),\n",
    "            'median_depth': detection.get('depth_information', {}).get('statistics', {}).get('median_depth'),\n",
    "            'min_depth': detection.get('depth_information', {}).get('statistics', {}).get('min_depth'),\n",
    "            'max_depth': detection.get('depth_information', {}).get('statistics', {}).get('max_depth'),\n",
    "            'std_depth': detection.get('depth_information', {}).get('statistics', {}).get('std_depth'),\n",
    "            'valid_pixels': detection.get('depth_information', {}).get('statistics', {}).get('valid_pixels'),\n",
    "            'total_pixels': detection.get('depth_information', {}).get('statistics', {}).get('total_pixels'),\n",
    "            'fill_ratio': detection.get('depth_information', {}).get('statistics', {}).get('fill_ratio'),\n",
    "            \n",
    "            # Center point depth\n",
    "            'center_depth_raw': detection.get('depth_information', {}).get('realsense_depth_data', {}).get('center_point', {}).get('depth_raw'),\n",
    "            'center_depth_meters': detection.get('depth_information', {}).get('realsense_depth_data', {}).get('center_point', {}).get('depth_meters'),\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def convert_depth_points_to_rows(data, source_file):\n",
    "    \"\"\"Convert depth sample points to rows for DataFrame\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    if not data or 'detections' not in data:\n",
    "        return rows\n",
    "    \n",
    "    for detection in data.get('detections', []):\n",
    "        sample_points = detection.get('depth_information', {}).get('realsense_depth_data', {}).get('sample_points', [])\n",
    "        \n",
    "        for point in sample_points:\n",
    "            row = {\n",
    "                'source_file': source_file,\n",
    "                'sample_id': data.get('sample_id'),\n",
    "                'timestamp': data.get('timestamp'),\n",
    "                'detection_id': detection.get('detection_id'),\n",
    "                'class_id': detection.get('class_id'),\n",
    "                'pixel_x': point.get('pixel_x'),\n",
    "                'pixel_y': point.get('pixel_y'),\n",
    "                'depth_raw': point.get('depth_raw'),\n",
    "                'depth_meters': point.get('depth_meters')\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def convert_summary_to_rows(data, source_file):\n",
    "    \"\"\"Convert session summary to rows for DataFrame\"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    detections = data.get('detections', [])\n",
    "    \n",
    "    # Count detections by class\n",
    "    class_counts = {}\n",
    "    total_confidence = 0\n",
    "    \n",
    "    for detection in detections:\n",
    "        class_id = detection.get('class_id')\n",
    "        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "        total_confidence += detection.get('confidence', 0)\n",
    "    \n",
    "    avg_confidence = total_confidence / len(detections) if detections else 0\n",
    "    \n",
    "    row = {\n",
    "        'source_file': source_file,\n",
    "        'sample_id': data.get('sample_id'),\n",
    "        'timestamp': data.get('timestamp'),\n",
    "        'session_start': data.get('session_start'),\n",
    "        'sample_count': data.get('sample_count'),\n",
    "        'camera_width': data.get('camera_info', {}).get('width'),\n",
    "        'camera_height': data.get('camera_info', {}).get('height'),\n",
    "        'depth_scale': data.get('camera_info', {}).get('depth_scale'),\n",
    "        'origin_x': data.get('camera_info', {}).get('origin_x'),\n",
    "        'origin_y': data.get('camera_info', {}).get('origin_y'),\n",
    "        'total_detections': len(detections),\n",
    "        'class_0_count': class_counts.get(0, 0),\n",
    "        'class_1_count': class_counts.get(1, 0),\n",
    "        'avg_confidence': avg_confidence,\n",
    "        \n",
    "        # Additional stats\n",
    "        'max_confidence': max([d.get('confidence', 0) for d in detections]) if detections else 0,\n",
    "        'min_confidence': min([d.get('confidence', 0) for d in detections]) if detections else 0,\n",
    "    }\n",
    "    \n",
    "    return [row]\n",
    "\n",
    "def find_json_files(folder_path, pattern=\"*.json\"):\n",
    "    \"\"\"Find all JSON files in folder and subfolders\"\"\"\n",
    "    folder_path = Path(folder_path)\n",
    "    \n",
    "    # Search in current folder and subfolders\n",
    "    json_files = []\n",
    "    json_files.extend(folder_path.glob(pattern))\n",
    "    json_files.extend(folder_path.glob(f\"**/{pattern}\"))\n",
    "    \n",
    "    return sorted(list(set(json_files)))  # Remove duplicates and sort\n",
    "\n",
    "def merge_json_folder_to_csv(folder_path, output_dir=\".\", pattern=\"*.json\", formats=['all']):\n",
    "    \"\"\"Merge all JSON files in folder to CSV\"\"\"\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = find_json_files(folder_path, pattern)\n",
    "    \n",
    "    if not json_files:\n",
    "        logger.warning(f\"No JSON files found in {folder_path}\")\n",
    "        return {}\n",
    "    \n",
    "    logger.info(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    # Initialize data containers\n",
    "    all_detections = []\n",
    "    all_depth_points = []\n",
    "    all_summaries = []\n",
    "    \n",
    "    # Process each file\n",
    "    processed_files = 0\n",
    "    failed_files = 0\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "        try:\n",
    "            data = load_json_data(json_file)\n",
    "            if data is None:\n",
    "                failed_files += 1\n",
    "                continue\n",
    "            \n",
    "            source_file = json_file.name\n",
    "            \n",
    "            # Convert data based on requested formats\n",
    "            if 'detections' in formats or 'all' in formats:\n",
    "                detection_rows = convert_detections_to_rows(data, source_file)\n",
    "                all_detections.extend(detection_rows)\n",
    "            \n",
    "            if 'depth_points' in formats or 'all' in formats:\n",
    "                depth_rows = convert_depth_points_to_rows(data, source_file)\n",
    "                all_depth_points.extend(depth_rows)\n",
    "            \n",
    "            if 'summary' in formats or 'all' in formats:\n",
    "                summary_rows = convert_summary_to_rows(data, source_file)\n",
    "                all_summaries.extend(summary_rows)\n",
    "            \n",
    "            processed_files += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {json_file}: {e}\")\n",
    "            failed_files += 1\n",
    "    \n",
    "    logger.info(f\"Processed: {processed_files}, Failed: {failed_files}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for output files\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Save DataFrames to CSV\n",
    "    if all_detections and ('detections' in formats or 'all' in formats):\n",
    "        df_detections = pd.DataFrame(all_detections)\n",
    "        output_file = output_dir / f\"merged_detections_{timestamp}.csv\"\n",
    "        df_detections.to_csv(output_file, index=False)\n",
    "        results['detections'] = output_file\n",
    "        logger.info(f\"Saved detections CSV: {output_file} ({len(df_detections)} rows)\")\n",
    "    \n",
    "    if all_depth_points and ('depth_points' in formats or 'all' in formats):\n",
    "        df_depth = pd.DataFrame(all_depth_points)\n",
    "        output_file = output_dir / f\"merged_depth_points_{timestamp}.csv\"\n",
    "        df_depth.to_csv(output_file, index=False)\n",
    "        results['depth_points'] = output_file\n",
    "        logger.info(f\"Saved depth points CSV: {output_file} ({len(df_depth)} rows)\")\n",
    "    \n",
    "    if all_summaries and ('summary' in formats or 'all' in formats):\n",
    "        df_summary = pd.DataFrame(all_summaries)\n",
    "        output_file = output_dir / f\"merged_summary_{timestamp}.csv\"\n",
    "        df_summary.to_csv(output_file, index=False)\n",
    "        results['summary'] = output_file\n",
    "        logger.info(f\"Saved summary CSV: {output_file} ({len(df_summary)} rows)\")\n",
    "    \n",
    "    # Generate processing report\n",
    "    report = {\n",
    "        'total_files_found': len(json_files),\n",
    "        'files_processed': processed_files,\n",
    "        'files_failed': failed_files,\n",
    "        'total_detections': len(all_detections),\n",
    "        'total_depth_points': len(all_depth_points),\n",
    "        'output_files': results\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Merge all JSON detection files in a folder to CSV')\n",
    "    parser.add_argument('folder_path', help='Path to folder containing JSON files')\n",
    "    parser.add_argument('--output_dir', '-o', default='.', help='Output directory (default: current directory)')\n",
    "    parser.add_argument('--pattern', '-p', default='*.json', help='File pattern to match (default: *.json)')\n",
    "    parser.add_argument('--format', '-f', nargs='+', \n",
    "                       choices=['detections', 'depth_points', 'summary', 'all'], \n",
    "                       default=['all'], help='Output formats (default: all)')\n",
    "    parser.add_argument('--recursive', '-r', action='store_true', \n",
    "                       help='Search subfolders recursively')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.recursive:\n",
    "        pattern = f\"**/{args.pattern}\"\n",
    "    else:\n",
    "        pattern = args.pattern\n",
    "    \n",
    "    print(f\"Merging JSON files from: {args.folder_path}\")\n",
    "    print(f\"Pattern: {pattern}\")\n",
    "    print(f\"Output directory: {args.output_dir}\")\n",
    "    print(f\"Formats: {args.format}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    report = merge_json_folder_to_csv(\n",
    "        folder_path=args.folder_path,\n",
    "        output_dir=args.output_dir,\n",
    "        pattern=pattern,\n",
    "        formats=args.format\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROCESSING REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Files found: {report['total_files_found']}\")\n",
    "    print(f\"Files processed: {report['files_processed']}\")\n",
    "    print(f\"Files failed: {report['files_failed']}\")\n",
    "    print(f\"Total detections: {report['total_detections']}\")\n",
    "    print(f\"Total depth points: {report['total_depth_points']}\")\n",
    "    print(f\"Output files: {list(report['output_files'].values())}\")\n",
    "\n",
    "# Example usage functions\n",
    "def quick_merge(folder_path, output_dir=\".\"):\n",
    "    \"\"\"Quick merge with default settings\"\"\"\n",
    "    return merge_json_folder_to_csv(folder_path, output_dir, formats=['all'])\n",
    "\n",
    "def merge_detections_only(folder_path, output_dir=\".\"):\n",
    "    \"\"\"Merge only detection data\"\"\"\n",
    "    return merge_json_folder_to_csv(folder_path, output_dir, formats=['detections'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 21:30:07,891 - WARNING - No JSON files found in ../training_data/8000\n"
     ]
    }
   ],
   "source": [
    "report = quick_merge('../training_data/8000', './output')\n",
    "# print(f\"Processed {report['files_proc/'essed']} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lworakan/Documents/GitHub/FIBOXVISION2025/main'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
